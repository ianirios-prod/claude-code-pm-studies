# Contexto do Projeto — DataFlow AI

## Quem sou eu
Sou um Technical AI Product Manager na DataFlow AI. Tenho background técnico e foco em produtos que usam machine learning e LLMs. Trabalho na interseção entre engenharia, dados e negócios.

## Sobre a Empresa
- **Produto:** DataFlow AI — plataforma de analytics inteligente para times de produto
- **Proposta de valor:** Transforma dados brutos de comportamento de usuário em insights acionáveis usando IA
- **Modelo:** B2B SaaS, foco em empresas de tecnologia (50–500 funcionários)
- **Stage:** Series B, ~150 funcionários, ARR de $8M
- **Stack principal:** Python (backend), React (frontend), PostgreSQL + BigQuery, modelos Claude e GPT-4 para features de IA

## Meu Time
- **Eng Lead:** Bruno Carvalho (focado em performance e escalabilidade)
- **Designer:** Camila Torres (especialista em data visualization)
- **Data Scientist:** Priya Sharma (modelos de ML e evals)
- **Stakeholder principal:** Rafael Mendes (CTO)
- **Customer Success:** Ana Lima

## Regras Sempre Seguir

### Comunicação
- Respostas em português, a menos que eu peça em inglês
- Documentos externos (PRDs para clientes, releases) em inglês
- Documentos internos em português
- Seja direto e conciso — não enrole

### Código
- Nunca commitar no GitHub sem me perguntar primeiro
- Sempre usar TypeScript quando for JavaScript
- Preferir soluções simples a over-engineering

### Pesquisa
- Quando fizer web search, sempre retornar as fontes
- Resumir em bullet points com no máximo 5 itens principais
- Destacar o que é relevante para um PM técnico (não só para devs)

### Documentos
- PRDs seguem o template em `prds/prd-example-ai-insights.md`
- Notas de reunião seguem o formato em `.claude/commands/meeting-notes.md`
- Análises de user research seguem o formato em `.claude/commands/user-research.md`

## Contexto Atual (Q1 2026)
- Sprint atual: #42
- OKR principal: Aumentar DAU em 40% até fim de Q1
- Feature em desenvolvimento: "AI Insights Feed" — feed personalizado de insights para cada usuário
- Maior blocker: latência das queries de LLM (média de 4.2s, meta < 2s)
